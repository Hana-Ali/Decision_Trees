{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Data #\n",
    "\n",
    "The code cell below contains code for reading in both the clean and noisy datasets (assumed to be stored in a local folder), and splitting these into training, testing, and validation sets. To load an unseen dataset, we have provided a commented out line of code, to which you need only pass in the local file path as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(data, test_proportion, val_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training, test, and validation sets according to the given \n",
    "        proportions.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): The datatset that is to be split, with shape (N,8)\n",
    "        test_proprotion (float): the desired proportion of test examples.\n",
    "                                 (0.0-1.0)\n",
    "        val_proprotion (float): the desired proportion of validation examples. \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator.\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (training_data, test_data, validation_data) \n",
    "               - training_data (np.ndarray): Training instances shape (n_train, 8)\n",
    "               - test_data (np.ndarray): Test instances shape (n_test, 8)\n",
    "               - validation_data (np.ndarray): Validation instances shape (n_val, 8)\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle indices and split into train, validation, and test.\n",
    "    shuffled_indices = random_generator.permutation(len(data))\n",
    "    n_test = round(len(data) * test_proportion)\n",
    "    n_val = round(len(data) * val_proportion)\n",
    "    n_train = len(data) - (n_test + n_val)\n",
    "\n",
    "    # Split the actual data using the shuffled indices.\n",
    "    training_data = data[shuffled_indices[:n_train]]\n",
    "    test_data = data[shuffled_indices[n_train : n_train + n_test]]\n",
    "    validation_data = data[shuffled_indices[n_train + n_test : ]]\n",
    "\n",
    "    return (training_data, test_data, validation_data)\n",
    "\n",
    "\n",
    "# Load datasets from local machine.\n",
    "clean_data = np.loadtxt(\"wifi_db/clean_dataset.txt\", dtype=int)\n",
    "noisy_data = np.loadtxt(\"wifi_db/noisy_dataset.txt\", dtype=float)\n",
    "\n",
    "# To load unseen data ...\n",
    "# unseen_data = np.loadtxt(\"wifi_db/...\", dtype=float)\n",
    "\n",
    "# Set default rng.\n",
    "rg = default_rng()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating Decision Trees (and pruning) #\n",
    "\n",
    "The section that follows contains code to implement and test a decision tree on both the noisy and clean datasets.\n",
    "\n",
    "### 2.1 Implementation ###\n",
    "\n",
    "The code cell that follows contains the implementation of a decision tree class (with pruning methods) defined over a recursively constructed set of interlinked nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\" Class representing a node data structure.\n",
    "\n",
    "    Attributes:\n",
    "        left (Node): Object reference to this node's left child.\n",
    "        right (Node): Object reference to this node's right child.\n",
    "        attribute (Node): The attribute index to be tested on (0-6).\n",
    "        value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point.\n",
    "                Leaf Node:     The label of any feature vector for which the tree path leads to this leaf node. \n",
    "\n",
    "        leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False) \n",
    "        label_count (int) : The number of training examples at a leaf node (always 0 for a decision node)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, left=None, right=None, attribute=None, value=None, leaf=None):\n",
    "        \"\"\" Constructor \n",
    "        Args:\n",
    "            left (Node): Object reference to this node's left child.\n",
    "            right (Node): Object reference to this node's right child.\n",
    "            attribute (Node): The attribute index to be tested on (0-6).\n",
    "            value (float): \n",
    "                Decision Node: The value at the indexed attribute, to be compared with the split point.\n",
    "                Leaf Node:     The label of any feature vector for which the tree path leads to this leaf node. \n",
    "\n",
    "            leaf (Boolean): Specifies whether the node is a leaf node (True) or not (False)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.leaf = leaf\n",
    "        self.label_count = 0\n",
    "    \n",
    "    # Getters -------------------------------------------------------------------------------\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_label_count(self):\n",
    "        return self.label_count\n",
    "    \n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.leaf\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Setters -------------------------------------------------------------------------------\n",
    "    def set_left(self,left):\n",
    "        self.left = left\n",
    "    \n",
    "    def set_right(self,right):\n",
    "        self.right = right\n",
    "    \n",
    "    def set_attribute(self,attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def set_leaf(self,leaf):\n",
    "        self.leaf = leaf\n",
    "\n",
    "    def set_label_count(self, label_count):\n",
    "        self.label_count = label_count\n",
    "\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"  Class representing a decision tree classifier.\n",
    "\n",
    "     Attributes:\n",
    "        unique_labels (array): Array of unique labels in the datatset.\n",
    "        root (Node): The root node of the tree (which contains an object reference to its children).\n",
    "        depth (int): Maximum path length from root to a leaf.\n",
    "        validation(numpy.ndarray): The dataset that is to be used to evaluate pruning performance, with shape (N,8).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, validation):\n",
    "        \"\"\" Constructor \n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): The datatset that is to be fit with a decision tree, with shape (N,8).\n",
    "                validation(numpy.ndarray): The dataset that is to be used to evaluate pruning performance, with shape (N,8).\n",
    "        \"\"\"\n",
    "        self.validation = validation\n",
    "        self.unique_labels = self.get_labels(dataset)\n",
    "        self.root, self.depth = self.decision_tree_learning(dataset, 0)\n",
    "        \n",
    "    \n",
    "    def get_labels(self, dataset):\n",
    "        \"\"\" Creates a list of the unique labels and assigns them to the unique_labels attribute.\n",
    "\n",
    "            Args:\n",
    "                dataset (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "        \"\"\"\n",
    "        return np.unique(dataset[:,-1:])\n",
    "    \n",
    "\n",
    "    def predict(self, samples):\n",
    "        \"\"\" Traverses the already built decision tree in order to make a classification label for a set of samples, relying on predict_single().\n",
    "\n",
    "            Args:\n",
    "                sample (numpy.ndarray): A dataset of unseen samples to be classified, with shape (N,8).\n",
    "\n",
    "             Returns:\n",
    "                labels (numpy.ndarray): Array containing label prediction of each sample, with shape (N,).\n",
    "        \"\"\"\n",
    "        return np.asarray([self.predict_single(self.root, sample) for sample in samples])\n",
    "\n",
    "\n",
    "    def predict_single(self, node, sample):\n",
    "        \"\"\" Traverses the already built decision tree in order to make a classification label for a single data sample, using recursion.\n",
    "\n",
    "            Args:\n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                sample (numpy.ndarray): A single data sample (row) to be classified, with shape (1,8).\n",
    "\n",
    "             Returns:\n",
    "                label (int): The predicted label of our sample.\n",
    "        \"\"\"\n",
    "\n",
    "        # Base case.\n",
    "        if node.is_leaf():\n",
    "            return node.get_value()\n",
    "\n",
    "        # Recursive case. Find the attribute we are splitting on and get the value.\n",
    "        node_attribute = node.get_attribute()\n",
    "        node_value = node.get_value()\n",
    "        sample_value = sample[node_attribute]\n",
    "        \n",
    "        # Compare.\n",
    "        if sample_value <= node_value:\n",
    "            return self.predict_single(node.get_left(), sample)\n",
    "        else:\n",
    "            return self.predict_single(node.get_right(), sample)\n",
    "\n",
    "\n",
    "    def decision_tree_learning(self, dataset, depth):\n",
    "        \"\"\" This function fits a binary decision tree to dataset, using recursion.\n",
    "        \n",
    "        Args: \n",
    "            dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "            depth (int): Maximum path length from root to a leaf. Always initialised to 0.\n",
    "        \n",
    "        Returns:\n",
    "            root (Node): The root node of the tree (which contains an object reference to its children).\n",
    "            depth (int): Maximum path length from root to a leaf.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Base Case: All dataset samples have the same label. \n",
    "        if self.have_same_label(dataset):  \n",
    "            return self.create_leaf_node(dataset), depth\n",
    "        \n",
    "        # Stated assumption that there are no inconsistent data points \n",
    "        # i.e. identical feature vectors having different classes, thus no need for second base case.\n",
    "\n",
    "        # 2. Inductive case: Further splitting to do.\n",
    "        split_attribute, split_value = self.FIND_SPLIT(dataset)\n",
    "        \n",
    "        # Instantiate node.\n",
    "        new_node = Node()    \n",
    "        new_node.set_value(split_value)\n",
    "        new_node.set_attribute(split_attribute)\n",
    "         \n",
    "        left_dataset, right_dataset = self.split_dataset(dataset, split_attribute, split_value)\n",
    "\n",
    "        # Recursive step (depth-first fashion).\n",
    "        l_branch, l_depth = self.decision_tree_learning(left_dataset, depth+1)\n",
    "        r_branch, r_depth = self.decision_tree_learning(right_dataset, depth+1)\n",
    "\n",
    "        new_node.set_left(l_branch)\n",
    "        new_node.set_right(r_branch)\n",
    "        depth = max(l_depth, r_depth)\n",
    "\n",
    "        return new_node, depth\n",
    "\n",
    "    \n",
    "    def shared_label(self, dataset):\n",
    "        \"\"\" Returns the label shared by every sample in dataset. \n",
    "            \n",
    "            *Function is called iff have_same_label() returns True.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "\n",
    "            Returns:\n",
    "                label (float): The label shared by all samples in dataset.\n",
    "        \"\"\"\n",
    "        label = dataset[0,-1]\n",
    "        return label\n",
    "\n",
    "\n",
    "    def have_same_label(self, dataset):\n",
    "        \"\"\" Checks if all samples in dataset share the same label.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "\n",
    "            Returns:\n",
    "                shared (Boolean): True  = all samples in dataset share the same label\n",
    "                                  False = variety of labels **(meaning it can be split further)\n",
    "\n",
    "        \"\"\"\n",
    "        labels = dataset[:, -1:]\n",
    "        shared = np.all(labels == labels[0])\n",
    "        return shared\n",
    "\n",
    "\n",
    "    def create_leaf_node(self, dataset_at_node):\n",
    "        \"\"\" Creates a leaf node with `value` set as the label, and determines the number of training samples that reach this leaf node.\n",
    "            Args: \n",
    "                dataset_at_node (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "            Returns:\n",
    "                node (Node): A leaf node that is to be appended to the node path.\n",
    "        \"\"\"\n",
    "        value = self.shared_label(dataset_at_node)\n",
    "        label_count = len(dataset_at_node) \n",
    "\n",
    "        # instantiate and set values\n",
    "        node = Node()\n",
    "        node.set_leaf(True)\n",
    "        node.set_value(value)\n",
    "        node.set_label_count(label_count)\n",
    "        return node\n",
    "\n",
    "\n",
    "    def FIND_SPLIT(self, dataset):\n",
    "        \"\"\" Chooses the attribute and the splitting value that results in the highest information gain.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "\n",
    "            Returns:\n",
    "                Tuple:\n",
    "                    attribute (int): The index of the optimal attribute to be split at.\n",
    "                    value: (float) :  The decision boundary of the attribute, which will split the incoming data.\n",
    "        \"\"\"\n",
    "        # (info_gain, attribute, split_value)\n",
    "        max_info_gain = (0, 0, 0) \n",
    "\n",
    "        # -1 so we don't loop over labels.\n",
    "        for col in range(dataset.shape[1] - 1): \n",
    "            sorted_by_col = dataset[dataset[:, col].argsort()] # Ascending order.\n",
    "\n",
    "            # Loop over rows of specific attribute (column) being evaluated.\n",
    "            for row in range(sorted_by_col.shape[0] - 1): \n",
    "\n",
    "                # Determine information gain for a trialled split value.\n",
    "                split_value = (sorted_by_col[row,col] + sorted_by_col[row+1, col]) / 2 # Midpoint\n",
    "                left_branch, right_branch = self.split_dataset(dataset, col, split_value)\n",
    "                info_gain = self.information_gain(dataset, left_branch, right_branch)\n",
    "\n",
    "                # Update maximal \n",
    "                if info_gain > max_info_gain[0]:\n",
    "                    max_info_gain = (info_gain, col, split_value)\n",
    "\n",
    "        # Select splitting attribute + splitting value.\n",
    "        attribute = max_info_gain[1]\n",
    "        value = max_info_gain[2]\n",
    "        return (attribute, value)\n",
    "\n",
    "\n",
    "    def entropy(self,dataset):\n",
    "        \"\"\" Returns the entropy of a given dataset.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset.\n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        for label in self.unique_labels:\n",
    "            proportion = self.p_label(dataset, label)\n",
    "            log_of_proportion = 0 if proportion == 0 else np.log2(proportion)  # Handle edge case.\n",
    "            sum += (proportion * log_of_proportion)\n",
    "\n",
    "        return -sum\n",
    "\n",
    "    def p_label(self, dataset, label):\n",
    "        \"\"\" Returns the proportion of the dataset that contains labels that are equal to a given label parameter.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "                label (int): The label we are evaluating the proportion of.\n",
    "\n",
    "            Returns:\n",
    "                entropy (float): The calculated entropy of the dataset. \n",
    "\n",
    "        \"\"\"\n",
    "        # Edge case: empty dataset.\n",
    "        return 0 if len(dataset) == 0 else sum([int(row[-1]) == int(label) for row in dataset]) / len(dataset)\n",
    "\n",
    "\n",
    "    def information_gain(self, dataset, left, right):\n",
    "        \"\"\" Returns the information gain from a given split of the dataset.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "                left (np.ndarray): The left split of dataset.\n",
    "                right (np.ndarray): The right split of dataset.\n",
    "\n",
    "            Returns:\n",
    "                information gain (float): The calculated information gain.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.entropy(dataset) - self.remainder(left,right)\n",
    "\n",
    "    def remainder(self, left, right):\n",
    "        \"\"\" Returns the entropy remaining after a given split. The information gain function subtracts this value \n",
    "            from the overall entropy in order to calculate the information gained. \n",
    "\n",
    "            Args: \n",
    "                left (np.ndarray): The left split of the dataset.\n",
    "                right (np.ndarray): The right split of the dataset.\n",
    "\n",
    "            Returns:\n",
    "                remainder (float): The calculated entropy remainder.\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # Components.\n",
    "        size_left = left.shape[0]\n",
    "        size_right = right.shape[0]\n",
    "        entropy_left = self.entropy(left)\n",
    "        entropy_right = self.entropy(right)\n",
    "\n",
    "        return ((size_left / (size_left + size_right)) * entropy_left) + ((size_right / (size_left + size_right))* entropy_right)\n",
    "\n",
    "    def split_dataset(self, dataset, attribute, split_value):\n",
    "        \"\"\" Splits the dataset into left + right datasets, from a given split value on a specific attribute.\n",
    "\n",
    "            Args: \n",
    "                dataset (np.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8)\n",
    "                attribute (int): The column index in dataset, of the specific attribute that will be split over.\n",
    "                split_value (float): The value that each row of data will compared with. \n",
    "                                     When the value of the row indexed at attribute <= split value, that row is moved into the left branch.\n",
    "                                     When the value of the row indexed at attribute > split value, that row is moved into the right branch.\n",
    "            Returns:\n",
    "                left (np.ndarray): The left split of the dataset.\n",
    "                right (np.ndarray): The right split of the dataset.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Implementational design to go left when equal to split_value.\n",
    "        left = dataset[((dataset[:,attribute] <= split_value))]\n",
    "        right = dataset[((dataset[:,attribute] > split_value))] \n",
    "\n",
    "        return left, right\n",
    "\n",
    "\n",
    "    def parents_of_leaves(self, node, parents):\n",
    "        \"\"\" Generates a list of all nodes in tree that are parents of two leaves.\n",
    "\n",
    "            Args: \n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "                parents (array): The list of parent nodes, initialised to an empty list.\n",
    "\n",
    "            The function does not return the parents list, it updates the parents list.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Base case. \n",
    "        if node.is_leaf():\n",
    "            return\n",
    "\n",
    "        # 2. Base case.\n",
    "        if node.get_left().is_leaf() and node.get_right().is_leaf():\n",
    "            parents.append(node) # Parent found\n",
    "            return\n",
    "        \n",
    "        # 3. Inductive Case (check children)\n",
    "        self.parents_of_leaves(node.get_left(), parents) \n",
    "        self.parents_of_leaves(node.get_right(), parents)\n",
    "        return \n",
    "\n",
    "\n",
    "    def majority(self, node):\n",
    "        \"\"\" Returns the majority classification label of a sub-tree rooted at `node`,\n",
    "            where the given node is a parent of two leaf nodes.\n",
    "\n",
    "            Args: \n",
    "                node (Node): Object reference to the current node in the decision tree.\n",
    "\n",
    "            Returns:\n",
    "                value (int): The majority class label of node's children.\n",
    "                node: (Node): The object reference to the child with the majority vote.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        left = node.get_left()\n",
    "        right = node.get_right()\n",
    "\n",
    "        # Number of training samples.\n",
    "        left_label_counts = left.get_label_count()\n",
    "        right_label_counts = right.get_label_count()\n",
    "\n",
    "        # Pick majority.\n",
    "        if left_label_counts >= right_label_counts:\n",
    "            return left.get_value(), left\n",
    "        else:\n",
    "            return right.get_value(), right\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, test_db, trained_tree):\n",
    "        \"\"\" Computes the accuracy of an already trained decision tree on the validation/test data set.\n",
    "\n",
    "            Args: \n",
    "                test_db (numpy.ndarray): Data instances (first 7 columns) + data labels (8th column), with shape (N,8).\n",
    "\n",
    "            Returns:\n",
    "                accuracy (float): The accuracy of the decision tree on the given datset.\n",
    "                                  (0.0-1.0)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # We are inside decision tree, so we don't explicitly use trained_tree, but include to be consistent with the specification.\n",
    "        y_predictions = self.predict(test_db)\n",
    "        y_gold = test_db[:,-1]\n",
    "\n",
    "        correct = sum([y_predictions[i] == y_gold[i] for i in range(len(test_db))])\n",
    "        accuracy = correct / len(test_db)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\" Attempts to prune the already built decision tree \n",
    "        (we say attempts because pruning may not improve performance, in which case it will not be applied).\n",
    "\n",
    "            This function takes no parameters and does not return anything. Rather it modifies the current decision tree object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the error before pruning.\n",
    "        prev_error = (1 - self.evaluate(self.validation, self))\n",
    "\n",
    "        parents_of_leaves = []\n",
    "        self.parents_of_leaves(self.root, parents_of_leaves)\n",
    "        \n",
    "        for parent in parents_of_leaves:\n",
    "\n",
    "            m_label, m_node = self.majority(parent)\n",
    "\n",
    "            # Backup value (in case prune needs to be undone).\n",
    "            old_value = parent.get_value()\n",
    "\n",
    "            # Prune tree.\n",
    "            parent.set_leaf(True)\n",
    "            parent.set_value(m_label)\n",
    "\n",
    "            # Test performance of pruned tree.\n",
    "            new_error = (1 - self.evaluate(self.validation, self))\n",
    "\n",
    "            if new_error <= prev_error:                     \n",
    "                # Success. Set children to null and try pruning again.\n",
    "                parent.set_left(None)\n",
    "                parent.set_right(None)\n",
    "                parent.set_label_count(m_node.get_label_count())\n",
    "                self.prune()\n",
    "                return\n",
    "            else:\n",
    "                # Revert to previous tree.\n",
    "                parent.set_leaf(False)\n",
    "                parent.set_value(old_value)\n",
    "                \n",
    "        self.update_tree_depth()\n",
    "        return\n",
    "\n",
    "\n",
    "    def update_tree_depth(self):\n",
    "        \"\"\"\" Updates the depth of this decision tree object by calling get_tree_depth() on the root node.\n",
    "             This function takes no parameters and does not return anything.\n",
    "        \"\"\"\n",
    "\n",
    "        self.depth = self.get_depth_at_node(self.root)\n",
    "        return\n",
    "    \n",
    "\n",
    "    def get_tree_depth(self):\n",
    "        \"\"\" Returns the depth of this decision tree (the longest path from root to leaf).\n",
    "            This function takes no arguments.\n",
    "            \n",
    "            Returns:\n",
    "                depth (int): The depth of the tree.\n",
    "        \"\"\"\n",
    "        return self.depth\n",
    "\n",
    "\n",
    "    def get_depth_at_node(self, node):\n",
    "        \"\"\" Computes the depth the of the tree. Assumes root node as\n",
    "            initial input.\n",
    "\n",
    "            Args:\n",
    "                node (Node): Initially the root node of the decision tree.\n",
    "\n",
    "            Returns:\n",
    "                depth (int): maximum path length from the root to a leaf.\n",
    "        \"\"\"\n",
    "        if node.is_leaf():\n",
    "            return 0\n",
    "        else:\n",
    "            l_depth = self.get_depth_at_node(node.get_left())\n",
    "            r_depth = self.get_depth_at_node(node.get_right())\n",
    "            return max(l_depth, r_depth) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Testing the implementation of the decision tree  ###\n",
    "\n",
    "The two code cells that follow build and then test the decision tree on both the noisy and clean datasets. In both cases the tree is built, and then tested for accuracy and depth. This is simply to test that the model builds and works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre pruning decision tree accuracy on the clean dataset.\n",
    "clean_data_train, clean_data_test, clean_data_val = split_dataset(clean_data, test_proportion=0.1, val_proportion=0,random_generator=rg)\n",
    "\n",
    "decision_tree = DecisionTree(clean_data_train, None)\n",
    "print(\"Test accuracy: {}\".format(decision_tree.evaluate(clean_data_test, decision_tree)))\n",
    "print(\"Depth: {}\".format(decision_tree.get_tree_depth()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre pruning decision tree accuracy on the noisy dataset.\n",
    "noisy_data_train, noisy_data_test, noisy_data_val = split_dataset(noisy_data, test_proportion=0.1, val_proportion=0,random_generator=rg)\n",
    "\n",
    "decision_tree = DecisionTree(noisy_data_train, None)\n",
    "print(\"Test accuracy: {}\".format(decision_tree.evaluate(noisy_data_test, decision_tree)))\n",
    "print(\"Depth: {}\".format(decision_tree.get_tree_depth()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (optional): Visualizing the tree ###\n",
    "\n",
    "Not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluation\n",
    "\n",
    "In the cell that follows we provide code for performing k-fold cross validation, nested k-fold cross validation and computing confusion matrices. These methods are then used in later sections and subsections to evaluate the decision tree performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # All functions in this cell have been inspired by and adapted from the lab solutions.\n",
    "\n",
    "def k_fold_split(k_folds, n_instances, random_generator=default_rng()):\n",
    "   \n",
    "    \"\"\" Split k_instances into n mutually exclusive splits at random.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds.\n",
    "        n_instances (int): Number of instances to split.\n",
    "        random_generator (np.random.Generator): A random generator.\n",
    "\n",
    "    Returns:\n",
    "        split_indices: a list (length k_folds). Each element in the list should contain a \n",
    "                       numpy array giving the indices of the instances in that split.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a random permutation of indices from 0 to n_instances.\n",
    "    shuffled_indices = random_generator.permutation(n_instances)\n",
    "\n",
    "    # Split shuffled indices into almost equal sized folds.\n",
    "    split_indices = np.array_split(shuffled_indices, k_folds)\n",
    "\n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def nested_k_fold_cross_val(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Generate train, test, and validation indices at each fold.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds.\n",
    "        n_instances (int): Total number of instances.\n",
    "        random_generator (np.random.Generator): A random generator.\n",
    "\n",
    "    Returns:\n",
    "        folds: a list of length k_folds. Each element in the list is a \n",
    "               list with three elements: \n",
    "                - train_indices (numpy.ndarray): The randomly shuffled training indices.\n",
    "                - test_indices (numpy.ndarray): The randomly shuffled test indices.\n",
    "                - val_indices (numpy.ndarray): The randomly shuffled val indices.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset into k folds.\n",
    "    split_indices = k_fold_split(k_folds, n_instances, random_generator)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(k_folds):\n",
    "        # Single test + training fold.\n",
    "        test_indices = split_indices[k]\n",
    "        # Train_indices = np.hstack(split_indices[:k] + split_indices[k+1:]).\n",
    "        remaining_training_blocks = set(range(k_folds)) - {k}\n",
    "\n",
    "        for i in remaining_training_blocks: \n",
    "            # Single validation fold.\n",
    "            val_indices = split_indices[i]\n",
    "            \n",
    "            # Remaining data assigned as training data.\n",
    "            train_indices = []\n",
    "            for j in (remaining_training_blocks - {i}):\n",
    "                train_indices.append(split_indices[j])\n",
    "            train_indices = np.array(train_indices, dtype=int).flatten()\n",
    "\n",
    "            folds.append([train_indices, np.array(test_indices), np.array(val_indices)])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "def k_fold_cross_val(k_folds, n_instances, random_generator=default_rng()):\n",
    "    \"\"\" Generate train, test and validation indices at each fold.\n",
    "    \n",
    "    Args:\n",
    "        k_folds (int): Number of folds.\n",
    "        n_instances (int): Total number of instances.\n",
    "        random_generator (np.random.Generator): A random generator.\n",
    "\n",
    "    Returns:\n",
    "        folds: a list of length k_folds. Each element in the list is a \n",
    "               list with three elements: \n",
    "                - train_indices (numpy.ndarray): The randomly shuffled training indices.\n",
    "                - test_indices (numpy.ndarray): The randomly shuffled test indices.\n",
    "                - val_indices (numpy.ndarray): The randomly shuffled val indices.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset into k splits.\n",
    "    split_indices = k_fold_split(k_folds, n_instances, random_generator)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(k_folds):\n",
    "        # Pick k as test.\n",
    "        test_indices = split_indices[k]\n",
    "\n",
    "        # Combine remaining splits as train.\n",
    "        train_indices = np.hstack(split_indices[:k] + split_indices[k+1:])\n",
    "\n",
    "        folds.append([train_indices, test_indices])\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "def confusion_matrix(y_gold, y_prediction, class_labels=None):\n",
    "    \"\"\" Compute the confusion matrix.\n",
    "        \n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth/gold standard labels.\n",
    "        y_prediction (np.ndarray): the predicted labels.\n",
    "        class_labels (np.ndarray): a list of unique class labels. \n",
    "                               Defaults to the union of y_gold and y_prediction.\n",
    "\n",
    "    Returns:\n",
    "        confusion (np.array): The confusion matrix with shape (C, C), where C is the number of classes. \n",
    "                              Rows are ground truth per class, columns are predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no class_labels are given, we obtain the set of unique class labels from \n",
    "    # the union of the ground truth annotation and the prediction.\n",
    "    if not class_labels:\n",
    "        class_labels = np.unique(np.concatenate((y_gold, y_prediction)))\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=int)\n",
    "\n",
    "    # For each correct class (row), \n",
    "    # compute how many instances are predicted for each class (columns).\n",
    "    for (i, label) in enumerate(class_labels):\n",
    "        \n",
    "        # Get predictions where the ground truth is the current class label.\n",
    "        indices = (y_gold == label)\n",
    "        gold = y_gold[indices]\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        # Quick way to get the counts per label.\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        # Convert the counts to a dictionary.\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        # Fill up the confusion matrix for the current row.\n",
    "        for (j, class_label) in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Evaluate clean and noisy data sets with k-fold-cross validation ##\n",
    "\n",
    "In the two cells that follow we provide code for computing confusion matrices and tree depth over the clean and noisy dataset using k-fold cross validation (with 10 folds). These are printed out in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_fold_evaluation(data, k_folds):\n",
    "    \"\"\" Performs k-fold cross validation on a provided dataset and computes the accuracies and confusion matrices at every fold, before and after pruning the tree.\n",
    "        \n",
    "    Args:\n",
    "        data (np.ndarray):  The datatset that is to be evaluated, with shape (N,8).\n",
    "        k_folds (int): The number of folds.\n",
    "\n",
    "    Returns:\n",
    "        confusions (list): A list where the elements are the individual confusion matrices of type (numpy.ndarray), at every fold.\n",
    "        depths (list): A list of integers, corresponding to the depth of each fold.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise as empty.\n",
    "    confusions = []\n",
    "    depths = []\n",
    "    \n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold_cross_val(k_folds, len(data), rg)):\n",
    "        print(\"fold {}\".format(i)) # Check progress.\n",
    "        \n",
    "        # Compute accuracy + confusion matrix before pruning.\n",
    "        decision_tree = DecisionTree(data[train_indices], None)\n",
    "        predictions = decision_tree.predict(data[test_indices])\n",
    "        confusion = confusion_matrix(data[test_indices][:,-1], predictions)\n",
    "        depth = decision_tree.get_tree_depth()\n",
    "\n",
    "        # Update accuracy list + confusion matrix list (before and after).\n",
    "        confusions.append(confusion)\n",
    "        depths.append(depth)\n",
    "        \n",
    "    return confusions, depths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute depth and confusion matrices for clean and noisy datasets over 10 folds.\n",
    "pre_confusions_clean, pre_depths_clean = k_fold_evaluation(clean_data, 10)\n",
    "pre_confusions_noisy, pre_depths_noisy = k_fold_evaluation(noisy_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cross validation classification metrics ##\n",
    "\n",
    "In the cell that follows we define code to compute the mean confusion matrix after cross validation. From this, we provide code to compute the accuracy, precision and recall. These are used in sections 3.2.1 and 3.2.1 to evaluate the performance of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import average\n",
    "\n",
    "def compute_metrics(confusions):\n",
    "    \"\"\"\" Computes the mean confusion matrix, accuracy, precision and recall\n",
    "         from a list of confusion matrices (obtained from cross validation).\n",
    "\n",
    "        Args: \n",
    "            confusions (list): A list of confusion matrices.\n",
    "\n",
    "         Returns:\n",
    "            mean_confusion (numpy.ndarray): The element-wise mean confusion matrix of the classifier, with shape (N,N) where N is number of class labels.\n",
    "            accuracy (float): The accuracy of the classifier.\n",
    "            precision (list): A list of precisions per label of the data.\n",
    "            recall (list): A list of recalls per label of the data.\n",
    "            f1_score (list): A list of f1 scores per label of the data.\n",
    "    \"\"\"\n",
    "    mean_confusion = np.mean(confusions, axis = 0)\n",
    "    accuracy = accuracy_from_confusion(mean_confusion)\n",
    "    precision = precision_from_confusion(mean_confusion)\n",
    "    recall = recall_from_confusion(mean_confusion)\n",
    "    f1_score = f1_score_from_confusion(mean_confusion)\n",
    "    return (mean_confusion, accuracy, precision, recall, f1_score)\n",
    "   \n",
    "\n",
    "def accuracy_from_confusion(confusion):\n",
    "    \"\"\"\" Computes the accuracy of the classifier from the given confusion matrix.\n",
    "\n",
    "         Args: \n",
    "            confusion (numpy.ndarray): A confusion matrix, with shape (N,N) where N is number of class labels.\n",
    "\n",
    "         Returns:\n",
    "            accuracy (float): The accuracy of the classifier.\n",
    "\n",
    "    \"\"\"\n",
    "    if np.sum(confusion) > 0:\n",
    "        return np.sum(np.diag(confusion)) / np.sum(confusion)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def precision_from_confusion(confusion):\n",
    "    \"\"\"\" Computes the precision of the classifier from the given confusion matrix.\n",
    "\n",
    "        Args: \n",
    "             confusion (numpy.ndarray): A confusion matrix, with shape (N,N) where N is number of class labels.\n",
    "\n",
    "         Returns:\n",
    "            precision (list): A list of precisions per label of the data.\n",
    "    \"\"\"\n",
    "    p = np.zeros((len(confusion), ))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[:, c]) > 0:\n",
    "            p[c] = confusion[c, c] / np.sum(confusion[:, c])\n",
    "    return p\n",
    "\n",
    "\n",
    "def recall_from_confusion(confusion):\n",
    "    \"\"\"\" Computes the recall of the classifier from the given confusion matrix.\n",
    "\n",
    "        Args: \n",
    "            confusion (numpy.ndarray): A confusion matrix, with shape (N,N) where N is number of class labels.\n",
    "\n",
    "         Returns:\n",
    "            recall (list): A list of recalls per label of the data.\n",
    "    \"\"\"\n",
    "    r = np.zeros((len(confusion), ))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[c, :]) > 0:\n",
    "            r[c] = confusion[c, c] / np.sum(confusion[c, :])\n",
    "    return r\n",
    "\n",
    "\n",
    "def f1_score_from_confusion(confusion):\n",
    "    \"\"\"\" Computes the f1 score of the classifier from the given confusion matrix.\n",
    "\n",
    "         Args: \n",
    "            confusion (numpy.ndarray): A confusion matrix, with shape (N,N) where N is number of class labels.\n",
    "\n",
    "         Returns:\n",
    "            f1_score (list): A list of f1 scores per label of the data.\n",
    "    \"\"\"\n",
    "    precisions = precision_from_confusion(confusion)\n",
    "    recalls = recall_from_confusion(confusion)\n",
    "\n",
    "    # Make sure they are of the same length.\n",
    "    assert len(precisions) == len(recalls)\n",
    "\n",
    "    f = np.zeros((len(precisions), ))\n",
    "    for c, (p, r) in enumerate(zip(precisions, recalls)):\n",
    "        if p + r > 0:\n",
    "            f[c] = 2 * p * r / (p + r)\n",
    "    return f\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Clean data ###\n",
    "\n",
    "In the cell that follows we print out the average depth and confusion matrix for an unpruned tree trained on the clean dataset using cross validation with 10 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_clean_depth_pre = np.mean(pre_depths_clean)\n",
    "mean_confusion, accuracy, precision, recall, f1_score = compute_metrics(pre_confusions_clean)\n",
    "print(\"mean confusion: \\n{}\".format(mean_confusion))\n",
    "print(\"\\naccuracy: {}\".format(accuracy))\n",
    "print(\"\\nprecision: {}\".format(precision))\n",
    "print(\"\\nrecall: {}\".format(recall))\n",
    "print(\"\\nf1 score: {}\".format(f1_score))\n",
    "print(\"\\naverage depth of tree: {}\".format(average_clean_depth_pre))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Noisy data ###\n",
    "\n",
    "In the cell that follows we print out the average depth and confusion matrix for an unpruned tree trained on the noisy dataset using cross validation with 10 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_noisy_depth_pre = np.mean(pre_depths_noisy)\n",
    "mean_confusion, accuracy, precision, recall, f1_score = compute_metrics(pre_confusions_noisy)\n",
    "print(\"mean confusion: \\n{}\".format(mean_confusion))\n",
    "print(\"\\naccuracy: {}\".format(accuracy))\n",
    "print(\"\\nprecision: {}\".format(precision))\n",
    "print(\"\\nrecall: {}\".format(recall))\n",
    "print(\"\\nf1 score: {}\".format(f1_score))\n",
    "print(\"\\naverage depth of tree: {}\".format(average_noisy_depth_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pruning (implemented in 2.1)\n",
    "\n",
    "The following section deals with tree evaluation post pruning. In particular, we perform nested k-fold cross validation and compare the mean confusion matrices, precision, recall, f1 score and depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluate clean and noisy datsets after pruning with nested k-fold cross validation ##\n",
    "\n",
    "The two code cells below compute the confusion matrices and depths over all 90 nested k-fold cross validaton sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_nested_evaluation(data, k_folds):\n",
    "    \"\"\" Performs k-fold cross validation on a provided dataset and computes the accuracies and confusion matrices at every fold, before and after pruning the tree.\n",
    "        \n",
    "    Args:\n",
    "        data (np.ndarray):  The datatset that is to be evaluated, with shape (N,8).\n",
    "        k_folds (int): The number of folds.\n",
    "\n",
    "    Returns:\n",
    "        confusions (list): A list where the elements are the individual confusion matrices of type (numpy.ndarray), at every fold.\n",
    "        depths (list): A list of integers, corresponding to the depth of each fold.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise as empty.\n",
    "    confusions = []\n",
    "    depths = []\n",
    "    \n",
    "    for i, (train_indices, test_indices, val_indices) in enumerate(nested_k_fold_cross_val(k_folds, len(data), rg)):\n",
    "        print(\"fold {}\".format(i))\n",
    "        \n",
    "        # Compute accuracy + confusion matrix before pruning.\n",
    "        decision_tree = DecisionTree(data[train_indices], data[val_indices])\n",
    "\n",
    "        decision_tree.prune()\n",
    "        \n",
    "        predictions = decision_tree.predict(data[test_indices])\n",
    "        confusion = confusion_matrix(data[test_indices][:,-1], predictions)\n",
    "        depth = decision_tree.get_tree_depth()\n",
    "\n",
    "        # Update accuracy list + confusion matrix list (before and after).\n",
    "        confusions.append(confusion)\n",
    "        depths.append(depth)\n",
    "        \n",
    "    return confusions, depths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_confusions_clean, pruned_depths_clean = k_fold_nested_evaluation(clean_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_confusions_noisy, pruned_depths_noisy = k_fold_nested_evaluation(noisy_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Cross Validation classification metrics\n",
    "\n",
    "In the two subsections that follow we print out the evaluation metrics after performing nested k-fold cross validation on the clean and noisy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Clean data\n",
    "\n",
    "The code cell below prints out the nested k-fold cross validation averages for the depth, confusion matrix, precision, recall and f1 score for the clean dataset after pruning the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "average_clean_depth_pruned = np.mean(pruned_depths_clean)\n",
    "mean_confusion, accuracy, precision, recall, f1_score = compute_metrics(pruned_confusions_clean)\n",
    "print(\"mean confusion: \\n{}\".format(mean_confusion))\n",
    "print(\"\\naccuracy: {}\".format(accuracy))\n",
    "print(\"\\nprecision: {}\".format(precision))\n",
    "print(\"\\nrecall: {}\".format(recall))\n",
    "print(\"\\nf1 score: {}\".format(f1_score))\n",
    "print(\"\\naverage depth of tree: {}\".format(average_clean_depth_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Noisy data\n",
    "\n",
    "The code cell below prints out the nested k-fold cross validation averages for the depth, confusion matrix, precision, recall and f1 score for the noisy dataset after pruning the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy\n",
    "average_noisy_depth = np.mean(pruned_depths_noisy)\n",
    "mean_confusion, accuracy, precision, recall, f1_score = compute_metrics(pruned_confusions_noisy)\n",
    "print(\"mean confusion: \\n{}\".format(mean_confusion))\n",
    "print(\"\\naccuracy: {}\".format(accuracy))\n",
    "print(\"\\nprecision: {}\".format(precision))\n",
    "print(\"\\nrecall: {}\".format(recall))\n",
    "print(\"\\nf1 score: {}\".format(f1_score))\n",
    "print(\"\\naverage depth of tree: {}\".format(average_noisy_depth))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
